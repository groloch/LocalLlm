# LocalLlm #

This python repository is meant to simplify the setup of AI assistants.\
This is still in a very early phase of development and not many functionalities are implemented.

## Installation ## 

Before installing this repository, make sure you have `llama-cpp-python` installed in your environment !\
To install `llama-cpp-python`, refer: https://github.com/abetlen/llama-cpp-python. \
To install this repository, you can simply clone it:\
`git clone https://github.com/groloch/LocalLlm.git`

## How to use ##

The notebook [here](notebook/local_llm_chat.ipynb) shows a straightforward way to use this repository !\
You can send messages to LLMs using the `<<` operator.\
You can generate text using the `>>` operator.

## Host you own chat assistant for free! ##

You can simply download the notebook [here](notebook/local_llm_chat.ipynb), import it on colab and run it !
